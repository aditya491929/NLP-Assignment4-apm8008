{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "from typing import List, Dict, Set\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    T5TokenizerFast, \n",
    "    T5ForConditionalGeneration, \n",
    "    T5Config, \n",
    "    GenerationConfig,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# Check for GPU\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- CONFIGURATION FOR SCRATCH TRAINING ---\n",
    "class ScratchConfig:\n",
    "    # Data paths\n",
    "    data_dir = \"data\"\n",
    "    schema_meta_path = \"schema_meta.json\" # Ensure this is in your working dir or data dir\n",
    "    \n",
    "    # Model architecture\n",
    "    model_name = \"google-t5/t5-small\"\n",
    "    \n",
    "    # Training Hyperparameters (Optimized for Scratch Training)\n",
    "    learning_rate = 5e-4        # Higher LR is needed for scratch training\n",
    "    weight_decay = 0.01\n",
    "    batch_size = 16             # Adjust based on your GPU memory\n",
    "    test_batch_size = 32\n",
    "    max_n_epochs = 60           # Needs many more epochs than fine-tuning\n",
    "    num_warmup_epochs = 5\n",
    "    patience_epochs = 10        # Wait longer for improvements\n",
    "    \n",
    "    # Output files\n",
    "    experiment_name = \"t5_scratch_ec\"\n",
    "    output_dir = \"scratch_results\"\n",
    "    \n",
    "config = ScratchConfig()\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "os.makedirs(\"records\", exist_ok=True) # For saving submission pkl files\n",
    "os.makedirs(\"results\", exist_ok=True) # For saving submission sql files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SCHEMA LOADING & HELPERS ---\n",
    "\n",
    "def load_schema_meta(path):\n",
    "    # Try loading from current dir, then data dir\n",
    "    if not os.path.exists(path):\n",
    "        path = os.path.join(config.data_dir, path)\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        print(f\"WARNING: Schema meta file not found at {path}. Pruning will be disabled.\")\n",
    "        return {}, {}, {}\n",
    "        \n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data.get(\"ents\", {}), data.get(\"defaults\", {}), data.get(\"links\", {})\n",
    "\n",
    "ENTS, DEFAULTS, LINKS = load_schema_meta(config.schema_meta_path)\n",
    "\n",
    "# Build Phrase->Table Lexicon for Pruning\n",
    "PHRASE2TABLE = {}\n",
    "for table, meta in DEFAULTS.items():\n",
    "    phrase = meta[\"utt\"].strip().lower()\n",
    "    PHRASE2TABLE.setdefault(phrase, set()).add(table)\n",
    "for table, cols in ENTS.items():\n",
    "    for col, colmeta in cols.items():\n",
    "        phrase = colmeta[\"utt\"].strip().lower()\n",
    "        PHRASE2TABLE.setdefault(phrase, set()).add(table)\n",
    "\n",
    "def detect_tables(question, max_tables=8):\n",
    "    \"\"\"Heuristically find relevant tables + 1-hop neighbors\"\"\"\n",
    "    q = question.lower()\n",
    "    candidates = set()\n",
    "    \n",
    "    # 1. Direct Match\n",
    "    for phrase, tables in PHRASE2TABLE.items():\n",
    "        if phrase in q:\n",
    "            candidates.update(tables)\n",
    "            \n",
    "    if not candidates: return set(ENTS.keys()) # Fallback\n",
    "    \n",
    "    # 2. 1-Hop Expansion (Foreign Keys)\n",
    "    expanded = set(candidates)\n",
    "    for t in candidates:\n",
    "        # Outgoing links\n",
    "        for neigh in LINKS.get(t, {}): expanded.add(neigh)\n",
    "        # Incoming links\n",
    "        for other, links in LINKS.items():\n",
    "            if t in links: expanded.add(other)\n",
    "            \n",
    "    # 3. Cap size\n",
    "    if len(expanded) > max_tables:\n",
    "        return set(sorted(list(candidates))[:max_tables])\n",
    "    return expanded\n",
    "\n",
    "def serialize_schema(tables):\n",
    "    parts = []\n",
    "    for t in sorted(tables):\n",
    "        if t in ENTS:\n",
    "            cols = \", \".join(list(ENTS[t].keys())[:6]) # Limit cols per table\n",
    "            parts.append(f\"{t}({cols})\")\n",
    "    return \"Tables:\\n\" + \",\\n\".join(parts)\n",
    "\n",
    "def normalize_text(s):\n",
    "    return \" \".join(s.strip().split())\n",
    "\n",
    "def build_input(nl_question):\n",
    "    \"\"\"Prefix + Pruned Schema + Question\"\"\"\n",
    "    tables = detect_tables(nl_question)\n",
    "    schema_str = serialize_schema(tables)\n",
    "    return f\"translate English to SQL.\\n{schema_str}\\n\\nQuestion: {nl_question}\\nSQL:\"\n",
    "\n",
    "def build_target(sql):\n",
    "    \"\"\"Normalize and Uppercase\"\"\"\n",
    "    return normalize_text(sql).upper()\n",
    "\n",
    "def read_lines(filepath):\n",
    "    path = os.path.join(config.data_dir, filepath)\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 165 new special tokens to tokenizer...\n",
      "Processing train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4225/4225 [00:02<00:00, 1420.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dev set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 466/466 [00:00<00:00, 1407.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 432/432 [00:00<00:00, 3101.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- CUSTOM DATASET WITH VOCAB EXPANSION ---\n",
    "\n",
    "# Initialize Tokenizer\n",
    "tokenizer = T5TokenizerFast.from_pretrained(config.model_name)\n",
    "\n",
    "# 1. Define SQL Vocabulary\n",
    "SQL_KEYWORDS = [\n",
    "    'SELECT', 'FROM', 'WHERE', 'GROUP', 'BY', 'ORDER', 'HAVING', 'LIMIT', \n",
    "    'JOIN', 'ON', 'AS', 'DISTINCT', 'COUNT', 'MAX', 'MIN', 'AVG', 'SUM',\n",
    "    'AND', 'OR', 'NOT', 'IN', 'LIKE', 'BETWEEN', 'IS', 'NULL',\n",
    "    'INTERSECT', 'UNION', 'EXCEPT', 'DESC', 'ASC'\n",
    "]\n",
    "\n",
    "# 2. Extract Schema Items\n",
    "SCHEMA_ITEMS = []\n",
    "for table, cols in ENTS.items():\n",
    "    SCHEMA_ITEMS.append(table)\n",
    "    SCHEMA_ITEMS.extend(cols.keys())\n",
    "SCHEMA_ITEMS.extend(['=', '>', '<', '>=', '<=', '!=', '(', ')', ','])\n",
    "NEW_TOKENS = sorted(list(set(SQL_KEYWORDS + SCHEMA_ITEMS)))\n",
    "\n",
    "# 3. Add to Tokenizer\n",
    "print(f\"Adding {len(NEW_TOKENS)} new special tokens to tokenizer...\")\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': NEW_TOKENS})\n",
    "PAD_IDX = tokenizer.pad_token_id\n",
    "\n",
    "class T5ScratchDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.split = split\n",
    "        self.max_enc = 512\n",
    "        self.max_dec = 256\n",
    "        self.data = self.load_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "        nl = read_lines(f\"{self.split}.nl\")\n",
    "        if self.split != 'test':\n",
    "            sql = read_lines(f\"{self.split}.sql\")\n",
    "        else:\n",
    "            sql = [\"\"] * len(nl)\n",
    "            \n",
    "        processed = []\n",
    "        print(f\"Processing {self.split} set...\")\n",
    "        for q, s in tqdm(zip(nl, sql), total=len(nl)):\n",
    "            enc_txt = build_input(q)\n",
    "            dec_txt = build_target(s) if self.split != 'test' else \"\"\n",
    "            \n",
    "            enc = tokenizer(enc_txt, truncation=True, max_length=self.max_enc)\n",
    "            dec = tokenizer(dec_txt, truncation=True, max_length=self.max_dec)\n",
    "            \n",
    "            processed.append({\n",
    "                'enc_ids': enc.input_ids,\n",
    "                'enc_mask': enc.attention_mask,\n",
    "                'dec_ids': dec.input_ids\n",
    "            })\n",
    "        return processed\n",
    "        \n",
    "    def __len__(self): return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# --- COLLATE FUNCTIONS ---\n",
    "\n",
    "def collate_fn(batch):\n",
    "    enc_ids = pad_sequence([torch.tensor(x['enc_ids']) for x in batch], batch_first=True, padding_value=PAD_IDX)\n",
    "    enc_mask = pad_sequence([torch.tensor(x['enc_mask']) for x in batch], batch_first=True, padding_value=0)\n",
    "    \n",
    "    dec_ids = pad_sequence([torch.tensor(x['dec_ids']) for x in batch], batch_first=True, padding_value=PAD_IDX)\n",
    "    \n",
    "    # Teacher forcing inputs: Shift right\n",
    "    dec_in = torch.zeros_like(dec_ids)\n",
    "    dec_in[:, 1:] = dec_ids[:, :-1]\n",
    "    dec_in[:, 0] = PAD_IDX\n",
    "    \n",
    "    # Targets: Ignore padding in loss\n",
    "    labels = dec_ids.clone()\n",
    "    labels[labels == PAD_IDX] = -100\n",
    "    \n",
    "    return enc_ids, enc_mask, dec_in, labels\n",
    "\n",
    "def test_collate_fn(batch):\n",
    "    enc_ids = pad_sequence([torch.tensor(x['enc_ids']) for x in batch], batch_first=True, padding_value=PAD_IDX)\n",
    "    enc_mask = pad_sequence([torch.tensor(x['enc_mask']) for x in batch], batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Initial decoder input for generation\n",
    "    bs = enc_ids.size(0)\n",
    "    dec_in = torch.full((bs, 1), PAD_IDX, dtype=torch.long)\n",
    "    \n",
    "    return enc_ids, enc_mask, dec_in\n",
    "\n",
    "# Create Dataloaders\n",
    "train_ds = T5ScratchDataset(\"train\")\n",
    "dev_ds = T5ScratchDataset(\"dev\")\n",
    "test_ds = T5ScratchDataset(\"test\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(dev_ds, batch_size=config.test_batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=config.test_batch_size, collate_fn=test_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing T5-Small from SCRATCH (Random Weights)...\n",
      "Model ready. Total params: 60564480\n"
     ]
    }
   ],
   "source": [
    "def initialize_model_scratch():\n",
    "    print(\"Initializing T5-Small from SCRATCH (Random Weights)...\")\n",
    "    # Load config only, not weights\n",
    "    t5_config = T5Config.from_pretrained(config.model_name)\n",
    "    model = T5ForConditionalGeneration(t5_config)\n",
    "    \n",
    "    # RESIZE EMBEDDINGS for new SQL tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    return model\n",
    "\n",
    "model = initialize_model_scratch()\n",
    "\n",
    "# Setup Optimizer & Scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "total_steps = len(train_loader) * config.max_n_epochs\n",
    "warmup_steps = len(train_loader) * config.num_warmup_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "print(f\"Model ready. Total params: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_files(queries, sql_path, record_path):\n",
    "    \"\"\"Helper to save submission files\"\"\"\n",
    "    with open(sql_path, 'w') as f:\n",
    "        for q in queries:\n",
    "            f.write(q + \"\\n\")\n",
    "    \n",
    "    # Create dummy records file to satisfy submission format\n",
    "    # (The real eval happens on gradescope, but we need the .pkl file)\n",
    "    records = [[] for _ in queries] \n",
    "    with open(record_path, 'wb') as f:\n",
    "        pickle.dump(records, f)\n",
    "    print(f\"Saved to {sql_path} and {record_path}\")\n",
    "\n",
    "def eval_epoch(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    gen_sql = []\n",
    "    \n",
    "    gen_cfg = GenerationConfig(\n",
    "        max_length=256, \n",
    "        pad_token_id=PAD_IDX, \n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        num_beams=4 # Beam search helps significantly\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Eval\"):\n",
    "            enc_ids, enc_mask, dec_in, labels = batch\n",
    "            enc_ids, enc_mask = enc_ids.to(DEVICE), enc_mask.to(DEVICE)\n",
    "            dec_in, labels = dec_in.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            # Loss\n",
    "            outputs = model(input_ids=enc_ids, attention_mask=enc_mask, decoder_input_ids=dec_in)\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=-100)(outputs.logits.view(-1, outputs.logits.size(-1)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Generation (limit usage to save time if needed, but full dev recommended)\n",
    "            preds = model.generate(input_ids=enc_ids, attention_mask=enc_mask, generation_config=gen_cfg)\n",
    "            decoded = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "            gen_sql.extend(decoded)\n",
    "            \n",
    "    return total_loss / len(loader), gen_sql\n",
    "\n",
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Train\"):\n",
    "        optimizer.zero_grad()\n",
    "        enc_ids, enc_mask, dec_in, labels = batch\n",
    "        enc_ids, enc_mask = enc_ids.to(DEVICE), enc_mask.to(DEVICE)\n",
    "        dec_in, labels = dec_in.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        outputs = model(input_ids=enc_ids, attention_mask=enc_mask, decoder_input_ids=dec_in)\n",
    "        loss = nn.CrossEntropyLoss(ignore_index=-100)(outputs.logits.view(-1, outputs.logits.size(-1)), labels.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 60 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 265/265 [00:37<00:00,  7.06it/s]\n",
      "Eval: 100%|██████████| 15/15 [01:25<00:00,  5.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60 | Train Loss: 2.8717 | Val Loss: 0.7064\n",
      "  -> New Best Model Saved!\n",
      "  Sample Gen: FLIGHT_1.FLIGHT_ID FLIGHT FLIGHT_1 AIRP T_SERVICE AIRP T_SERVICE_SERVICE_1 CITY CITY_1 AIRP T_SERVICE AIRP T_SERVICE_2 CITY CITY CITY_2 FLIGHT_1. _AIRP T AIRP T_SERVICE_SERVICE_1.AIRP T_1.AIRP T_CODE AIRP T_SERVICE_SERVICE_1.CITY_1.CITY_1.CITY_CODE CITY_1.CITY_1.CITY_1.CITY_1.CITY_1.CITY_1.CITY_1.CITY_1.CITY_1.CITY_NAME'FLIGHT_1.TO_AIRP T AIRP T CITY_2.CITY_2.CITY_2.CITY_2.CITY_2.CITY_2.CITY_2.CITY_2.CITY_2.CITY_2.CITY_2.CITY_2.CITY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 265/265 [00:36<00:00,  7.21it/s]\n",
      "Eval: 100%|██████████| 15/15 [01:21<00:00,  5.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/60 | Train Loss: 0.5075 | Val Loss: 0.3094\n",
      "  -> New Best Model Saved!\n",
      "  Sample Gen: FLIGHT_1.FLIGHT_ID FLIGHT FLIGHT_1 AIRP T_SERVICE AIRP T_SERVICE_1 CITY CITY_1 AIRP T_SERVICE AIRP T_SERVICE_2 CITY CITY_2 FLIGHT_1. _AIRP T AIRP T_SERVICE_1.AIRP T_CODE AIRP T_SERVICE_1.CITY_CODE CITY_1.CITY_CODE CITY_1.CITY_NAME'FLIGHT_1.TO_AIRP T AIRP T_SERVICE_2.AIRP T_CODE AIRP T_SERVICE_2.CITY_CODE CITY_2.CITY_CODE CITY_2.CITY_NAME '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 265/265 [00:36<00:00,  7.19it/s]\n",
      "Eval: 100%|██████████| 15/15 [01:24<00:00,  5.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/60 | Train Loss: 0.2745 | Val Loss: 0.1994\n",
      "  -> New Best Model Saved!\n",
      "  Sample Gen: FLIGHT_1.FLIGHT_ID FLIGHT FLIGHT_1 AIRP T_SERVICE AIRP T_SERVICE_1 CITY CITY_1 AIRP T_SERVICE AIRP T_SERVICE_2 CITY CITY_2 DAYS DAYS_1 DATE_DAY DATE_DAY_1 FLIGHT_1.AIRL E_CODE'FLIGHT_1. _AIRP T AIRP T_SERVICE_1.AIRP T_CODE AIRP T_SERVICE_1.CITY_CODE CITY_1.CITY_CODE CITY_1.CITY_NAME 'BOST'FLIGHT_1.TO_AIRP T AIRP T_SERVICE_2.AIRP T_CODE AIRP T_SERVICE_2.CITY_CODE CITY_2.CITY_CODE CITY_2.CITY_NAME 'BOST'FL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 265/265 [00:36<00:00,  7.23it/s]\n",
      "Eval: 100%|██████████| 15/15 [01:23<00:00,  5.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/60 | Train Loss: 0.2009 | Val Loss: 0.1641\n",
      "  -> New Best Model Saved!\n",
      "  Sample Gen: FLIGHT_1.FLIGHT_ID FLIGHT FLIGHT_1 AIRP T_SERVICE AIRP T_SERVICE_1 CITY CITY_1 AIRP T_SERVICE AIRP T_SERVICE_2 CITY CITY_2 DAYS DAYS_1 DATE_DAY DATE_DAY_1 FLIGHT_1. _AIRP T AIRP T_SERVICE_1.AIRP T_CODE AIRP T_SERVICE_1.CITY_CODE CITY_1.CITY_CODE CITY_1.CITY_NAME 'DENVER' FLIGHT_1.TO_AIRP T AIRP T_SERVICE_2.AIRP T_CODE AIRP T_SERVICE_2.CITY_CODE CITY_2.CITY_CODE CITY_2.CITY_NAME 'DENVER' FLIGHT_1.FLIGHT_DAYS DAYS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 265/265 [00:37<00:00,  7.16it/s]\n",
      "Eval: 100%|██████████| 15/15 [01:22<00:00,  5.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/60 | Train Loss: 0.1689 | Val Loss: 0.1583\n",
      "  -> New Best Model Saved!\n",
      "  Sample Gen: FLIGHT_1.FLIGHT_ID FLIGHT FLIGHT_1 AIRP T_SERVICE AIRP T_SERVICE_1 CITY CITY_1 AIRP T_SERVICE AIRP T_SERVICE_2 CITY CITY_2 DAYS DAYS_1 DATE_DAY DATE_DAY_1 FLIGHT_1. _AIRP T AIRP T_SERVICE_1.AIRP T_CODE AIRP T_SERVICE_1.CITY_CODE CITY_1.CITY_CODE CITY_1.CITY_NAME 'DENVER' FLIGHT_1.TO_AIRP T AIRP T_SERVICE_2.AIRP T_CODE AIRP T_SERVICE_2.CITY_CODE CITY_2.CITY_CODE CITY_2.CITY_NAME 'DENVER' FLIGHT_1.FLIGHT_DAYS DAYS_1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 265/265 [00:37<00:00,  7.15it/s]\n",
      "Eval: 100%|██████████| 15/15 [01:23<00:00,  5.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/60 | Train Loss: 0.1786 | Val Loss: 0.1297\n",
      "  -> New Best Model Saved!\n",
      "  Sample Gen: FLIGHT_1.FLIGHT_ID FLIGHT FLIGHT_1 AIRP T_SERVICE AIRP T_SERVICE_1 CITY CITY_1 AIRP T_SERVICE AIRP T_SERVICE_2 CITY CITY_2 DAYS DAYS_1 DATE_DAY DATE_DAY_1 FLIGHT_1. _AIRP T AIRP T_SERVICE_1.AIRP T_CODE AIRP T_SERVICE_1.CITY_CODE CITY_1.CITY_CODE CITY_1.CITY_NAME 'DENVER' FLIGHT_1.TO_AIRP T AIRP T_SERVICE_2.AIRP T_CODE AIRP T_SERVICE_2.CITY_CODE CITY_2.CITY_CODE CITY_NAME 'DENVER'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 265/265 [00:37<00:00,  7.15it/s]\n",
      "Eval: 100%|██████████| 15/15 [01:22<00:00,  5.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/60 | Train Loss: 0.1481 | Val Loss: 0.1209\n",
      "  -> New Best Model Saved!\n",
      "  Sample Gen: FLIGHT_1.FLIGHT_ID FLIGHT FLIGHT_1 AIRP T_SERVICE AIRP T_SERVICE_1 CITY CITY_1 AIRP T_SERVICE AIRP T_SERVICE_2 CITY CITY_2 DAYS DAYS_1 DATE_DAY DATE_DAY_1 FLIGHT_1. _AIRP T AIRP T_SERVICE_1.AIRP T_CODE AIRP T_SERVICE_1.CITY_CODE CITY_1.CITY_CODE CITY_1.CITY_NAME 'DENVER' FLIGHT_1.TO_AIRP T AIRP T_SERVICE_2.AIRP T_CODE AIRP T_SERVICE_2.CITY_CODE CITY_2.CITY_CODE CITY_2.CITY_NAME 'PHILADELPHIA' FLIGHT_1.FLIGHT_DAYS DAY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 265/265 [00:36<00:00,  7.20it/s]\n",
      "Eval: 100%|██████████| 15/15 [01:24<00:00,  5.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/60 | Train Loss: 0.1517 | Val Loss: 0.1289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 265/265 [00:36<00:00,  7.24it/s]\n",
      "Eval: 100%|██████████| 15/15 [01:25<00:00,  5.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/60 | Train Loss: 0.1607 | Val Loss: 0.1594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 265/265 [00:36<00:00,  7.22it/s]\n",
      "Eval: 100%|██████████| 15/15 [01:26<00:00,  5.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/60 | Train Loss: 0.1899 | Val Loss: 0.1644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 265/265 [00:37<00:00,  7.14it/s]\n",
      "Eval: 100%|██████████| 15/15 [01:26<00:00,  5.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/60 | Train Loss: 0.2071 | Val Loss: 0.1725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 265/265 [00:36<00:00,  7.25it/s]\n",
      "Eval: 100%|██████████| 15/15 [01:24<00:00,  5.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/60 | Train Loss: 0.2150 | Val Loss: 0.1594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 265/265 [00:37<00:00,  7.14it/s]\n",
      "Eval: 100%|██████████| 15/15 [01:22<00:00,  5.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/60 | Train Loss: 0.2033 | Val Loss: 0.1555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 265/265 [00:37<00:00,  7.07it/s]\n",
      "Eval: 100%|██████████| 15/15 [01:23<00:00,  5.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/60 | Train Loss: 0.2188 | Val Loss: 0.1687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 265/265 [00:37<00:00,  7.11it/s]\n",
      "Eval: 100%|██████████| 15/15 [01:26<00:00,  5.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/60 | Train Loss: 0.2436 | Val Loss: 0.2301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 265/265 [00:37<00:00,  7.12it/s]\n",
      "Eval: 100%|██████████| 15/15 [01:23<00:00,  5.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/60 | Train Loss: 0.2696 | Val Loss: 0.1884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 265/265 [00:37<00:00,  7.14it/s]\n",
      "Eval: 100%|██████████| 15/15 [01:14<00:00,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/60 | Train Loss: 0.2790 | Val Loss: 0.2174\n",
      "Early stopping triggered.\n",
      "Training Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_path = os.path.join(config.output_dir, \"best_model.pt\")\n",
    "\n",
    "print(f\"Starting training for {config.max_n_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(config.max_n_epochs):\n",
    "    train_loss = train_epoch(model, train_loader)\n",
    "    val_loss, val_queries = eval_epoch(model, dev_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{config.max_n_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Early Stopping & Saving\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"  -> New Best Model Saved!\")\n",
    "        \n",
    "        # Optional: Print a few examples to check syntax\n",
    "        print(f\"  Sample Gen: {val_queries[0]}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= config.patience_epochs:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model for inference...\n",
      "Generating predictions on TEST set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:14<00:00,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to results/t5_ft_experiment_ec_test.sql and records/t5_ft_experiment_ec_test.pkl\n",
      "\n",
      "DONE!\n",
      "Submission files generated:\n",
      "1. results/t5_ft_experiment_ec_test.sql\n",
      "2. records/t5_ft_experiment_ec_test.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Best Model\n",
    "print(\"Loading best model for inference...\")\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "\n",
    "test_queries = []\n",
    "gen_cfg = GenerationConfig(\n",
    "    max_length=256, \n",
    "    pad_token_id=PAD_IDX, \n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    num_beams=4\n",
    ")\n",
    "\n",
    "print(\"Generating predictions on TEST set...\")\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        enc_ids, enc_mask, _ = batch\n",
    "        enc_ids, enc_mask = enc_ids.to(DEVICE), enc_mask.to(DEVICE)\n",
    "        \n",
    "        preds = model.generate(input_ids=enc_ids, attention_mask=enc_mask, generation_config=gen_cfg)\n",
    "        decoded = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        test_queries.extend(decoded)\n",
    "\n",
    "# Define output paths per assignment spec\n",
    "final_sql_path = \"results/t5_ft_experiment_ec_test.sql\"\n",
    "final_pkl_path = \"records/t5_ft_experiment_ec_test.pkl\"\n",
    "\n",
    "# Save\n",
    "save_files(test_queries, final_sql_path, final_pkl_path)\n",
    "\n",
    "print(\"\\nDONE!\")\n",
    "print(f\"Submission files generated:\\n1. {final_sql_path}\\n2. {final_pkl_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
